{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b533a861",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T03:16:40.538514Z",
     "start_time": "2023-11-12T03:16:39.533678Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import csv\n",
    "import re\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dfb51264",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T03:16:40.632445Z",
     "start_time": "2023-11-12T03:16:40.541519Z"
    }
   },
   "outputs": [],
   "source": [
    "#get east-coast data\n",
    "df_wp = pd.read_csv('news/east-coast/dc/articles.csv')\n",
    "df_md = pd.read_csv('news/east-coast/md/articles.csv')\n",
    "df_nyt = pd.read_csv('news/east-coast/nyc/articles.csv')\n",
    "#get west-coast data\n",
    "df_la = pd.read_csv('news/west-coast/la/articles.csv')\n",
    "df_sf = pd.read_csv('news/west-coast/sf/articles.csv')\n",
    "df_ws = pd.read_csv('news/west-coast/ws/articles.csv')\n",
    "#get mid-west data\n",
    "df_cst = pd.read_csv('news/mid-west/chicago/articles.csv')\n",
    "df_mst = pd.read_csv('news/mid-west/minneapolis/articles.csv')\n",
    "df_dn = pd.read_csv('news/mid-west/dallas/articles.csv')\n",
    "df_hc = pd.read_csv('news/mid-west/houston/articles.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7958f505",
   "metadata": {},
   "source": [
    "#### will compare the sentiment of the three regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b437d79c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T03:16:41.723554Z",
     "start_time": "2023-11-12T03:16:41.709560Z"
    }
   },
   "outputs": [],
   "source": [
    "west_coast_df = pd.concat([df_la, df_sf, df_ws], axis=0).reset_index(drop = True)\n",
    "east_coast_df = pd.concat([df_wp, df_md, df_nyt], axis=0).reset_index(drop = True)\n",
    "mid_west_df = pd.concat([df_cst, df_mst, df_dn, df_hc], axis=0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b43be20",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T03:16:42.111550Z",
     "start_time": "2023-11-12T03:16:42.092707Z"
    }
   },
   "outputs": [],
   "source": [
    "# removing speacial characters,symbols, white spaces without altering the decimal numbers\n",
    "mid_west_df['article'] = mid_west_df['article'].apply(lambda text: ' '.join(re.sub(r'[^a-zA-Z0-9\\s.]', '', text).split()))\n",
    "east_coast_df['article'] = east_coast_df['article'].apply(lambda text: ' '.join(re.sub(r'[^a-zA-Z0-9\\s.]', '', text).split()))\n",
    "west_coast_df['article'] = west_coast_df['article'].apply(lambda text: ' '.join(re.sub(r'[^a-zA-Z0-9\\s.]', '', text).split()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5775bda4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T03:16:44.437747Z",
     "start_time": "2023-11-12T03:16:42.789186Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\blahb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\blahb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\blahb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\blahb\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7811c06e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T03:16:44.453287Z",
     "start_time": "2023-11-12T03:16:44.441858Z"
    }
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "702d69ab",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T03:16:48.907164Z",
     "start_time": "2023-11-12T03:16:46.325690Z"
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = word_tokenize(text)\n",
    "\n",
    "    # Removing stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
    "\n",
    "    # Stemming (using Porter Stemmer)\n",
    "    stemmer = PorterStemmer()\n",
    "    stemmed_tokens = [stemmer.stem(word) for word in filtered_tokens]\n",
    "\n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in stemmed_tokens]\n",
    "\n",
    "    return ' '.join(filtered_tokens)\n",
    "\n",
    "\n",
    "mid_west_df['article'] = mid_west_df['article'].apply(preprocess_text)\n",
    "east_coast_df['article'] = east_coast_df['article'].apply(preprocess_text)\n",
    "west_coast_df['article'] = west_coast_df['article'].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1172b2dd-61b9-4c6a-95bf-4d9568a26947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contractions\n",
    "from word2number import w2n\n",
    "import unicodedata\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "def remove_URL(text):\n",
    "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    text =  url.sub(r'',text)\n",
    "    return text\n",
    "\n",
    "def strip_html_tags(text):\n",
    "    \"\"\"remove html tags from text\"\"\"\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    stripped_text = soup.get_text(separator=\" \")\n",
    "    return stripped_text\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "    return text\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
    "    return text\n",
    "    \n",
    "# Extra white spaces\n",
    "def remove_whitespace(text):\n",
    "    \"\"\"remove extra whitespaces from text\"\"\"\n",
    "    text = text.strip()\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "def expand_text(text):\n",
    "    # creating an empty list\n",
    "    expanded_words = []   \n",
    "    for word in text.split():\n",
    "        # using contractions.fix to expand the shortened words\n",
    "        expanded_words.append(contractions.fix(word))  \n",
    "       \n",
    "    expanded_text = ' '.join(expanded_words)\n",
    "    return expanded_text\n",
    "\n",
    "def word_to_number(text):\n",
    "    res = w2n.word_to_num(test_str)\n",
    "    return res\n",
    "\n",
    "def correct_spellings(text):\n",
    "    spell = SpellChecker()\n",
    "    \n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word is None or word == \"\":\n",
    "            continue\n",
    "        elif word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "\n",
    "\n",
    "def remove_stopwords(sentence, word_tokens):\n",
    "    my_stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
    "    tokens = sentence.split(\" \")\n",
    "    tokens_filtered= [word for word in word_tokens if not word in my_stopwords]\n",
    "    return (\" \").join(tokens_filtered)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1951744b-b7de-42b9-8a4a-ce6cc91375d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_doc(doc, URL_stripping=True, html_stripping=True, contraction_expansion=True,\n",
    "                     accented_char_removal=True, text_lower_case=True, \n",
    "                     text_lemmatization=True, special_char_removal=True, \n",
    "                     stopword_removal=True , emoji_removal=True,\n",
    "                     spelling_correction = True, word_to_num=False):\n",
    "    \n",
    "    \n",
    "    normalized_document = []\n",
    "    # Stip URL's\n",
    "    if URL_stripping:\n",
    "        doc = remove_URL(doc)\n",
    "    # strip HTML\n",
    "    if html_stripping:\n",
    "        doc = strip_html_tags(doc)\n",
    "    # remove accented characters\n",
    "    if accented_char_removal:\n",
    "        doc = remove_accented_chars(doc)\n",
    "    # expand contractions    \n",
    "    if contraction_expansion:\n",
    "        doc = expand_text(doc)\n",
    "    # lowercase the text    \n",
    "    if text_lower_case:\n",
    "        doc = doc.lower()\n",
    "    # Word to numbers\n",
    "    if word_to_num:\n",
    "        doc = word_to_num(doc)\n",
    "    # remove extra newlines\n",
    "    doc = re.sub(r'[\\r|\\n|\\r\\n]+', ' ',doc)\n",
    "    # remove special characters    \n",
    "    if special_char_removal:\n",
    "        doc = remove_special_characters(doc)\n",
    "    # remove stopwords\n",
    "    if stopword_removal:\n",
    "        text_tokens = word_tokenize(doc)\n",
    "        doc = remove_stopwords(doc, text_tokens)\n",
    "    \n",
    "    # remove extra whitespace\n",
    "    doc = re.sub(' +', ' ', doc)\n",
    "    \n",
    "    normalized_document.append(doc)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8960edae-0527-4e0e-9aa2-e2f9b1849bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_west_df['article'] = mid_west_df['article'].apply(lambda x: normalize_doc(x))\n",
    "east_coast_df['article'] = east_coast_df['article'].apply(lambda x: normalize_doc(x))\n",
    "west_coast_df['article'] = west_coast_df['article'].apply(lambda x: normalize_doc(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7a63964",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-12T03:16:48.922665Z",
     "start_time": "2023-11-12T03:16:48.910144Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 15 entries, 0 to 14\n",
      "Data columns (total 5 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   title        15 non-null     object\n",
      " 1   article      15 non-null     object\n",
      " 2   date         15 non-null     object\n",
      " 3   news_source  15 non-null     object\n",
      " 4   region       15 non-null     object\n",
      "dtypes: object(5)\n",
      "memory usage: 728.0+ bytes\n"
     ]
    }
   ],
   "source": [
    "mid_west_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e6af6f-1291-44c1-ac0a-23f46e28602a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Combine all the preprocessed text into a single string\n",
    "combined_text = ' '.join(mid_west_df['article'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
    "\n",
    "# Display the Word Cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Mid West Region\")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f988e7-dd67-4ebc-8490-7c5071af1038",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_text = ' '.join(east_coast_df['article'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
    "\n",
    "# west_coast_df\n",
    "# Display the Word Cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"East Coast\")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce5a3906-7686-4dd1-b49c-c85d2f9e9175",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "combined_text = ' '.join(west_coast_df['article'])\n",
    "\n",
    "# Create a WordCloud object\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(combined_text)\n",
    "\n",
    "# west_coast_df\n",
    "# Display the Word Cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"West Coast\")\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2260a-fe17-4eaf-8f6f-07351b220707",
   "metadata": {},
   "outputs": [],
   "source": [
    "mid_west_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1829d72-05df-4559-819a-ab3ac6e5d2e8",
   "metadata": {},
   "source": [
    "## Date cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6cfa9dd-6083-4e42-8ef4-d747d6c91164",
   "metadata": {},
   "source": [
    "#### Mid West Region"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff51e219-cf81-4089-aeb2-b144d84bc57c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "def convert_date(date_str):\n",
    "    # Define different date formats\n",
    "    formats = [\n",
    "        \"%Y-%m-%d %H:%M:%S\",  # 2021-04-05 12:30:27\n",
    "        \"%I:%M %p on %b %d, %Y %Z\",  # 5:38 AM on Dec 28, 2022 CST\n",
    "        \"%B %d, %Y\",  # June 15, 2017\n",
    "        \"%b. %d, %Y\",  # Jan. 20, 2023\n",
    "        \"%B %d, %Y\"  # July 3, 2023\n",
    "    ]\n",
    "\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            # Try to parse the date string with each format\n",
    "            dt = datetime.strptime(date_str, fmt)\n",
    "            return dt.strftime(\"%m-%d-%Y\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    # If no format matches, return the original string or a placeholder\n",
    "    return date_str\n",
    "\n",
    "def custom_date_parser_mid_west(date_str):\n",
    "    # Check if the date matches the specific pattern\n",
    "    match = re.match(r'(\\d+:\\d+\\s[APM]{2})\\son\\s([A-Za-z]+\\s\\d+,\\s\\d{4})\\s[A-Z]{3}', date_str)\n",
    "    if match:\n",
    "        # Extract the time and date parts\n",
    "        time_part = match.group(1)\n",
    "        date_part = match.group(2)\n",
    "\n",
    "        # Combine and parse the date\n",
    "        full_date_str = f'{date_part} {time_part}'\n",
    "        parsed_date = datetime.strptime(full_date_str, \"%b %d, %Y %I:%M %p\")\n",
    "        return parsed_date.strftime(\"%m-%d-%Y\")\n",
    "    else:\n",
    "        # Handle other date formats here, or return a placeholder\n",
    "        return date_str\n",
    "        \n",
    "mid_west_df['converted_date'] = mid_west_df['date'].apply(convert_date)\n",
    "mid_west_df['converted_date'] = mid_west_df['converted_date'].apply(custom_date_parser_mid_west)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "759a6acf-f74a-48cd-a59d-2d4cee42c403",
   "metadata": {},
   "source": [
    "#### East coast Date cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83247ace-9991-40ee-a848-1b8b8b84bb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "east_coast_df['date'].iloc[3] = east_coast_df['date'][3].split('|')[1]\n",
    "east_coast_df['date'].iloc[3] = east_coast_df['date'].iloc[3].replace('Published ', '')\n",
    "east_coast_df['date'].iloc[10] = east_coast_df['date'].iloc[10][east_coast_df['date'].iloc[10].find('Updated') + len('Updated'): ]\n",
    "east_coast_df['date'].iloc[13] = east_coast_df['date'].iloc[13][east_coast_df['date'].iloc[13].find('Updated') + len('Updated'): ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b4bea3d-33b9-4c4a-bacd-a352e3f62ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_standard_format(date_str):\n",
    "    # Remove words like 'Published' and 'at', and timezone information\n",
    "    cleaned_date_str = re.sub(r'(Published\\s+|\\sat\\s+|\\s+[A-Z]{3})', ' ', date_str).strip()\n",
    "\n",
    "    # Define date formats to try\n",
    "    formats = [\n",
    "        \"%Y-%m-%d %H:%M:%S\",   # 2022-08-09 10:54:00\n",
    "        \"%B %d, %Y\",           # June 23, 2023\n",
    "        \"%b. %d, %Y\",          # Jan. 15, 2021\n",
    "        \"%b %d, %Y\"            # Aug. 9, 2022\n",
    "    ]\n",
    "\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(cleaned_date_str, fmt)\n",
    "            return parsed_date.strftime(\"%m-%d-%Y\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    # Return a placeholder if no format matches\n",
    "    return date_str\n",
    "\n",
    "def custom_date_parser_east_coast(date_str):\n",
    "    # Extract the date and time components without the timezone\n",
    "    match = re.match(r'(\\w+ \\d+, \\d+) at (\\d+:\\d+ [apm.]+)', date_str)\n",
    "    if not match:\n",
    "        return date_str\n",
    "\n",
    "    date_part, time_part = match.groups()\n",
    "\n",
    "    # Remove periods from the time part (for 'a.m.' and 'p.m.')\n",
    "    time_part = time_part.replace('.', '')\n",
    "\n",
    "    # Combine the date and time parts and parse\n",
    "    full_date_str = f'{date_part} {time_part}'\n",
    "    parsed_date = datetime.strptime(full_date_str, \"%B %d, %Y %I:%M %p\")\n",
    "    return parsed_date.strftime(\"%m-%d-%Y\")\n",
    "    \n",
    "east_coast_df['converted_date'] = east_coast_df['date'].apply(lambda x: convert_to_standard_format(x))\n",
    "east_coast_df['converted_date'] = east_coast_df['converted_date'].apply(custom_date_parser_east_coast)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b65d5fce-25e6-481e-ac12-bda5c3bec718",
   "metadata": {},
   "outputs": [],
   "source": [
    "east_coast_df['converted_date']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a2ebc8-9b8a-4128-a858-8165841ea318",
   "metadata": {},
   "source": [
    "#### West coast date cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7938f0-ab73-42de-ab29-a5eadff64a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_coast_df['date'] = west_coast_df['date'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9096b3d-2263-450e-a941-e74c498582d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_date_format(date_str):\n",
    "    # Remove timezone and time, if present\n",
    "    date_str = re.sub(r'(\\d{1,2}:\\d{2} [APM]{2} [A-Z]{2}| \\d{1,2} [APM]{2} [A-Z]{2})', '', date_str).strip()\n",
    "\n",
    "    # Define date formats\n",
    "    formats = [\n",
    "        \"%b. %d, %Y\",  # \"Sept. 17, 2022\"\n",
    "        \"%B %d, %Y\",   # \"March 21, 2019\"\n",
    "    ]\n",
    "\n",
    "    for fmt in formats:\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(date_str, fmt)\n",
    "            return parsed_date.strftime(\"%m-%d-%Y\")\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    # Return a placeholder if no format matches\n",
    "    return date_str\n",
    "\n",
    "\n",
    "west_coast_df['converted_date'] = west_coast_df['date'].apply(lambda x: convert_date_format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f8ee451-3fce-47f4-be40-cc445c56e6a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from datetime import datetime\n",
    "\n",
    "def custom_date_parser_regex(date_str):\n",
    "    # Mapping for extended month abbreviations to standard three-letter abbreviations\n",
    "    month_mapping = {\n",
    "        'Jan.': 'Jan', 'Feb.': 'Feb', 'Mar.': 'Mar', 'Apr.': 'Apr', 'May.': 'May',\n",
    "        'Jun.': 'Jun', 'Jul.': 'Jul', 'Aug.': 'Aug', 'Sep.': 'Sep', 'Sept.': 'Sep',\n",
    "        'Oct.': 'Oct', 'Nov.': 'Nov', 'Dec.': 'Dec'\n",
    "    }\n",
    "\n",
    "    # Define the regex pattern for the date format\n",
    "    pattern = r'([A-Za-z]+\\.?) (\\d{1,2}), (\\d{4})'\n",
    "    match = re.match(pattern, date_str)\n",
    "\n",
    "    if match:\n",
    "        month_abbr, day, year = match.groups()\n",
    "\n",
    "        # Convert month abbreviation to a standard three-letter abbreviation\n",
    "        month_std_abbr = month_mapping.get(month_abbr, None)\n",
    "        if not month_std_abbr:\n",
    "            return date_str\n",
    "\n",
    "        # Parse the date using the standard abbreviation\n",
    "        try:\n",
    "            parsed_date = datetime.strptime(f\"{month_std_abbr} {day} {year}\", \"%b %d %Y\")\n",
    "            return parsed_date.strftime(\"%m-%d-%Y\")\n",
    "        except ValueError:\n",
    "            return date_str\n",
    "    else:\n",
    "        return date_str\n",
    "\n",
    "west_coast_df['converted_date'] = west_coast_df['converted_date'].apply(lambda x: custom_date_parser_regex(x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc2cd6f-5980-4fcd-8d97-f5cf448eebc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "west_coast_df.drop('date', axis=1, inplace=True)\n",
    "east_coast_df.drop('date', axis=1, inplace=True)\n",
    "mid_west_df.drop('date', axis=1, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
